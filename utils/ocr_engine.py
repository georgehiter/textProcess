"""
OCRæ™ºèƒ½å¼•æ“
é›†æˆé…ç½®ç®¡ç†ã€è¯­è¨€æ£€æµ‹ã€æ–‡æ¡£åˆ†æåŠŸèƒ½
ä¸ºæ‰«æç‰ˆPDFè½¬æ¢æä¾›æ™ºèƒ½OCRé…ç½®é€‰æ‹©
"""

import re
import numpy as np
from typing import Dict, Any, Tuple, List
import cv2
from PIL import Image
from langdetect import detect, detect_langs, LangDetectException
import pytesseract

# è®¾ç½®Tesseractè·¯å¾„ï¼ˆWindowsï¼‰
pytesseract.pytesseract.tesseract_cmd = r"C:\Program Files\Tesseract-OCR\tesseract.exe"


class OCREngine:
    """OCRæ™ºèƒ½å¼•æ“ - é›†æˆé…ç½®ã€è¯­è¨€æ£€æµ‹ã€æ–‡æ¡£åˆ†æ"""

    # ==================== é…ç½®å¸¸é‡ ====================

    # åŸºäºæœ€ä½³å®è·µçš„ä¸­æ–‡OCRé»˜è®¤é…ç½®
    CHINESE_OCR_BEST_PRACTICES = {
        # å­¦æœ¯è®ºæ–‡é…ç½® - é«˜è´¨é‡è¦æ±‚
        "academic_paper": {
            "lang": "chi_sim+eng",
            "psm": 1,  # è‡ªåŠ¨é¡µé¢åˆ†å‰²+OSD
            "dpi": 400,  # é«˜åˆ†è¾¨ç‡
            "name": "academic_paper",
            "description": "å­¦æœ¯è®ºæ–‡ã€é«˜è´¨é‡æ–‡æ¡£",
        },
        # æŠ€æœ¯æ–‡æ¡£é…ç½® - å¹³è¡¡æ€§èƒ½
        "technical_doc": {
            "lang": "chi_sim+eng",
            "psm": 6,  # ç»Ÿä¸€æ–‡æœ¬å—
            "dpi": 300,  # æ ‡å‡†åˆ†è¾¨ç‡
            "name": "technical_doc",
            "description": "æŠ€æœ¯æ–‡æ¡£ã€æ ‡å‡†æ–‡æ¡£",
        },
        # è¡¨æ ¼æ–‡æ¡£é…ç½® - ä¿æŒç»“æ„
        "table_document": {
            "lang": "chi_sim+eng",
            "psm": 13,  # åŸå§‹è¡Œæ¨¡å¼
            "dpi": 300,  # æ ‡å‡†åˆ†è¾¨ç‡
            "name": "table_document",
            "description": "è¡¨æ ¼ã€ç»“æ„åŒ–æ–‡æ¡£",
        },
        # çº¯ä¸­æ–‡æ–‡æ¡£é…ç½®
        "chinese_only": {
            "lang": "chi_sim",
            "psm": 1,  # è‡ªåŠ¨é¡µé¢åˆ†å‰²+OSD
            "dpi": 300,  # æ ‡å‡†åˆ†è¾¨ç‡
            "name": "chinese_only",
            "description": "çº¯ä¸­æ–‡æ–‡æ¡£",
        },
        # å¿«é€Ÿå¤„ç†é…ç½® - æ‰¹é‡å¤„ç†
        "fast_processing": {
            "lang": "chi_sim+eng",
            "psm": 6,  # ç»Ÿä¸€æ–‡æœ¬å—
            "dpi": 200,  # è¾ƒä½åˆ†è¾¨ç‡
            "name": "fast_processing",
            "description": "å¿«é€Ÿå¤„ç†ã€æ‰¹é‡æ–‡æ¡£",
        },
    }

    # é»˜è®¤ä¸­æ–‡OCRé…ç½® - ä½¿ç”¨æŠ€æœ¯æ–‡æ¡£é…ç½®ä½œä¸ºé»˜è®¤
    DEFAULT_CHINESE_OCR_CONFIG = CHINESE_OCR_BEST_PRACTICES["technical_doc"]

    # è‹±æ–‡OCRé»˜è®¤é…ç½® - ä½¿ç”¨æœ€ä½³å®è·µçš„æ ‡å‡†é…ç½®
    DEFAULT_ENGLISH_OCR_CONFIG = {
        "lang": "eng",
        "psm": 6,  # ç»Ÿä¸€æ–‡æœ¬å—
        "dpi": 400,  # æ ‡å‡†åˆ†è¾¨ç‡
        "name": "english_standard",
        "description": "æ ‡å‡†è‹±æ–‡æ–‡æ¡£é…ç½®",
    }

    # ä¿ç•™åŸæœ‰é…ç½®ç”¨äºå…¼å®¹æ€§
    SCAN_OCR_CONFIGS = [
        # é…ç½®1ï¼šè‡ªåŠ¨é¡µé¢åˆ†å‰²ï¼Œä¸­è‹±æ–‡æ··åˆ
        {"lang": "chi_sim+eng", "psm": 1, "dpi": 300, "name": "auto_page"},
        # é…ç½®2ï¼šç»Ÿä¸€æ–‡æœ¬å—ï¼Œä¸­è‹±æ–‡æ··åˆ
        {"lang": "chi_sim+eng", "psm": 6, "dpi": 300, "name": "uniform_block"},
        # é…ç½®3ï¼šä»…ä¸­æ–‡è¯†åˆ«
        {"lang": "chi_sim", "psm": 1, "dpi": 300, "name": "chinese_only"},
    ]

    # è¯­è¨€æ£€æµ‹é…ç½®
    LANGUAGE_DETECTION_CONFIG = {
        "confidence_threshold": 0.7,  # æé«˜ç½®ä¿¡åº¦é˜ˆå€¼
        "min_text_length_for_detection": 50,  # å¢åŠ æœ€å°æ–‡æœ¬é•¿åº¦
        "fallback_to_mixed": True,  # æ£€æµ‹å¤±è´¥æ—¶æ˜¯å¦å›é€€åˆ°æ··åˆæ¨¡å¼
        "sample_ocr_config": {  # ç”¨äºè¯­è¨€æ£€æµ‹çš„å¿«é€ŸOCRé…ç½®
            "lang": "chi_sim+eng",
            "psm": 1,
            "dpi": 150,  # é™ä½DPIä»¥æå‡é€Ÿåº¦
        },
    }

    # ç®€åŒ–çš„è¯­è¨€OCRé…ç½®æ˜ å°„ - ç›´æ¥ä½¿ç”¨æœ€ä½³å®è·µé…ç½®
    LANGUAGE_OCR_CONFIGS = {
        "zh": [DEFAULT_CHINESE_OCR_CONFIG],  # ä¸­æ–‡æ–‡æ¡£ä½¿ç”¨é»˜è®¤é…ç½®
        "en": [DEFAULT_ENGLISH_OCR_CONFIG],  # è‹±æ–‡æ–‡æ¡£ä½¿ç”¨é»˜è®¤é…ç½®
        "mixed": [DEFAULT_CHINESE_OCR_CONFIG],  # æ··åˆè¯­è¨€ä½¿ç”¨é»˜è®¤é…ç½®
        "fallback": [DEFAULT_CHINESE_OCR_CONFIG],  # å›é€€åˆ°é»˜è®¤é…ç½®
    }

    # å›¾åƒå¢å¼ºé…ç½® - åŸºäºæœ€ä½³å®è·µä¼˜åŒ–
    SCAN_IMAGE_ENHANCEMENT = {
        "contrast_factor": 1.3,  # å¯¹æ¯”åº¦å¢å¼ºå› å­
        "sharpness": True,  # æ˜¯å¦å¯ç”¨é”åŒ–
        "denoise": True,  # æ˜¯å¦å¯ç”¨å»å™ª
        "scale_factor": 2.5,  # å›¾åƒç¼©æ”¾å› å­
        "clahe_clip_limit": 2.0,  # CLAHEå¯¹æ¯”åº¦é™åˆ¶
        "clahe_tile_size": (8, 8),  # CLAHEç“¦ç‰‡å¤§å°
        # æ–°å¢ï¼šåŸºäºæœ€ä½³å®è·µçš„é¢„å¤„ç†é€‰é¡¹
        "remove_alpha_channel": True,  # ç§»é™¤é€æ˜é€šé“
        "add_border": True,  # æ·»åŠ ç™½è‰²è¾¹æ¡†
        "border_size": "10x10",  # è¾¹æ¡†å¤§å°
    }

    # æ–‡æœ¬æ¸…ç†é…ç½®
    SCAN_TEXT_CLEANING = {
        "remove_extra_spaces": True,  # ç§»é™¤å¤šä½™ç©ºæ ¼
        "normalize_newlines": True,  # è§„èŒƒåŒ–æ¢è¡Œ
        "fix_common_errors": True,  # ä¿®å¤å¸¸è§é”™è¯¯
    }

    # æ–‡ä»¶å¤¹é…ç½®
    FOLDER_CONFIG = {
        "ocr_uploads_folder": "ocr_uploads",  # OCRä¸Šä¼ æ–‡ä»¶å¤¹
        "outputs_folder": "outputs",  # è¾“å‡ºæ–‡ä»¶å¤¹
        "supported_extensions": [".pdf"],  # æ”¯æŒçš„æ–‡ä»¶æ‰©å±•å
    }

    # è¾“å‡ºé…ç½®
    SCAN_OUTPUT_CONFIG = {
        "default_encoding": "utf-8",  # é»˜è®¤ç¼–ç 
        "include_page_breaks": True,  # åŒ…å«é¡µé¢åˆ†éš”ç¬¦
    }

    # è¾“å‡ºæ ¼å¼é…ç½®
    OUTPUT_FORMAT_CONFIG = {
        "default_format": "md",  # é»˜è®¤è¾“å‡ºæ ¼å¼ï¼šmd æˆ– txt
        "include_metadata": True,  # æ˜¯å¦åŒ…å«å…ƒæ•°æ®
        "page_headers": True,  # æ˜¯å¦åŒ…å«é¡µé¢æ ‡é¢˜
        "processing_info": True,  # æ˜¯å¦åŒ…å«å¤„ç†ä¿¡æ¯
        "separator_line": "---",  # é¡µé¢åˆ†éš”ç¬¦
    }

    # æ–‡æ¡£ç±»å‹æ£€æµ‹é…ç½®
    DOCUMENT_TYPE_DETECTION_CONFIG = {
        # è¡¨æ ¼æ£€æµ‹é…ç½®
        "table": {
            "line_threshold": 50,  # ç›´çº¿æ£€æµ‹é˜ˆå€¼
            "min_line_length": 50,  # æœ€å°ç›´çº¿é•¿åº¦
            "max_line_gap": 10,  # æœ€å¤§ç›´çº¿é—´éš™
            "horizontal_ratio_threshold": 0.3,  # æ°´å¹³çº¿æ¯”ä¾‹é˜ˆå€¼
            "vertical_ratio_threshold": 0.2,  # å‚ç›´çº¿æ¯”ä¾‹é˜ˆå€¼
            "min_lines": 8,  # æœ€å°çº¿æ¡æ•°
        },
        # å­¦æœ¯è®ºæ–‡æ£€æµ‹é…ç½®
        "academic": {
            "keywords": [
                "abstract",
                "introduction",
                "conclusion",
                "references",
                "bibliography",
                "method",
                "methodology",
                "results",
                "discussion",
                "figure",
                "table",
                "equation",
                "æ‘˜è¦",
                "å¼•è¨€",
                "ç»“è®º",
                "å‚è€ƒæ–‡çŒ®",
                "æ–¹æ³•",
                "ç»“æœ",
                "è®¨è®º",
            ],
            "citation_patterns": [
                r"\[\d+\]",  # [1], [2], etc.
                r"\(\d{4}\)",  # (2023), (2024), etc.
                r"et al\.",  # et al.
                r"å‚è€ƒæ–‡çŒ®",  # ä¸­æ–‡å¼•ç”¨
            ],
            "section_patterns": [
                r"^\d+\.\s+\w+",  # 1. Introduction
                r"^\d+\.\d+\s+\w+",  # 1.1 Background
                r"^[A-Z][a-z]+\s*$",  # Abstract, Introduction
            ],
            "math_patterns": [
                r"[Î±-Ï‰Î‘-Î©]",  # å¸Œè…Šå­—æ¯
                r"[âˆ‘âˆ«âˆâˆšâˆ]",  # æ•°å­¦ç¬¦å·
                r"\\[a-zA-Z]+",  # LaTeXå‘½ä»¤
            ],
            "keyword_weight": 1.0,  # å…³é”®è¯æƒé‡
            "citation_weight": 2.0,  # å¼•ç”¨æƒé‡
            "section_weight": 1.5,  # ç« èŠ‚æƒé‡
            "math_weight": 1.5,  # æ•°å­¦å…¬å¼æƒé‡
            "threshold": 3.0,  # æ€»åˆ†é˜ˆå€¼
        },
        # çº¯ä¸­æ–‡æ£€æµ‹é…ç½®
        "chinese_only": {
            "chinese_ratio_threshold": 0.8,  # ä¸­æ–‡å­—ç¬¦æ¯”ä¾‹é˜ˆå€¼
            "english_ratio_threshold": 0.1,  # è‹±æ–‡å­—ç¬¦æ¯”ä¾‹é˜ˆå€¼
            "min_chinese_chars": 20,  # æœ€å°ä¸­æ–‡å­—ç¬¦æ•°
        },
        # æ‰¹é‡å¤„ç†æ£€æµ‹é…ç½®
        "batch": {
            "file_count_threshold": 10,  # æ–‡ä»¶æ•°é‡é˜ˆå€¼
            "processing_mode": "batch",  # å¤„ç†æ¨¡å¼
        },
    }

    # ==================== è¯­è¨€æ£€æµ‹æ–¹æ³• ====================

    @staticmethod
    def detect_language(text: str) -> Tuple[str, float]:
        """
        æ£€æµ‹æ–‡æœ¬çš„ä¸»è¦è¯­è¨€ç±»å‹ï¼ˆå¢å¼ºç‰ˆï¼‰

        Args:
            text: å¾…æ£€æµ‹çš„æ–‡æœ¬

        Returns:
            Tuple[str, float]: (è¯­è¨€ä»£ç , ç½®ä¿¡åº¦)
        """
        if (
            not text
            or len(text.strip())
            < OCREngine.LANGUAGE_DETECTION_CONFIG["min_text_length_for_detection"]
        ):
            return "unknown", 0.0

        try:
            # 1. å­—ç¬¦ç»Ÿè®¡éªŒè¯
            char_distribution = OCREngine.analyze_language_distribution(text)

            # 2. å¦‚æœä¸­æ–‡å­—ç¬¦æ¯”ä¾‹å¾ˆé«˜ï¼Œç›´æ¥è¿”å›ä¸­æ–‡
            if char_distribution.get("chinese", 0) > 0.6:
                return "zh", 0.9

            # 3. å¦‚æœè‹±æ–‡å­—ç¬¦æ¯”ä¾‹å¾ˆé«˜ï¼Œç›´æ¥è¿”å›è‹±æ–‡
            if char_distribution.get("english", 0) > 0.6:
                return "en", 0.9

            # 4. ä½¿ç”¨langdetectè¿›è¡Œæ£€æµ‹
            main_lang = detect(text)

            # 5. è·å–æ‰€æœ‰è¯­è¨€çš„ç½®ä¿¡åº¦
            lang_probs = detect_langs(text)

            # 6. æ‰¾åˆ°ä¸»è¦è¯­è¨€çš„ç½®ä¿¡åº¦
            confidence = 0.0
            for lang_prob in lang_probs:
                if lang_prob.lang == main_lang:
                    confidence = lang_prob.prob
                    break

            # 7. è¯­è¨€ä»£ç æ˜ å°„
            lang_mapping = {
                "zh": "zh",  # ä¸­æ–‡
                "zh-cn": "zh",
                "zh-tw": "zh",
                "zh-hk": "zh",
                "en": "en",  # è‹±æ–‡
                "ja": "ja",  # æ—¥æ–‡
                "ko": "ko",  # éŸ©æ–‡
            }

            detected_lang = lang_mapping.get(main_lang, main_lang)

            # 8. ç‰¹æ®Šå¤„ç†ï¼šå¦‚æœæ£€æµ‹ä¸ºéŸ©æ–‡ä½†ä¸­æ–‡å­—ç¬¦æ¯”ä¾‹è¾ƒé«˜ï¼Œé‡æ–°åˆ¤æ–­
            if detected_lang == "ko" and char_distribution.get("chinese", 0) > 0.3:
                # å¯èƒ½æ˜¯ä¸­æ–‡è¢«è¯¯åˆ¤ä¸ºéŸ©æ–‡
                if confidence < 0.8:  # ç½®ä¿¡åº¦ä¸å¤Ÿé«˜æ—¶
                    return "zh", 0.7

            # 9. å¦‚æœæ£€æµ‹ä¸ºä¸­æ–‡ä½†ç½®ä¿¡åº¦è¾ƒä½ï¼Œè¿›è¡ŒäºŒæ¬¡éªŒè¯
            if detected_lang == "zh" and confidence < 0.6:
                # æ£€æŸ¥æ˜¯å¦çœŸçš„æ˜¯ä¸­æ–‡
                if char_distribution.get("chinese", 0) < 0.3:
                    return "unknown", confidence

            return detected_lang, confidence

        except LangDetectException:
            return "unknown", 0.0

    @staticmethod
    def detect_mixed_language(text: str) -> Tuple[str, float]:
        """
        æ£€æµ‹æ··åˆè¯­è¨€æ–‡æ¡£

        Args:
            text: å¾…æ£€æµ‹çš„æ–‡æœ¬

        Returns:
            Tuple[str, float]: (è¯­è¨€ç±»å‹, ç½®ä¿¡åº¦)
        """
        if (
            not text
            or len(text.strip())
            < OCREngine.LANGUAGE_DETECTION_CONFIG["min_text_length_for_detection"]
        ):
            return "unknown", 0.0

        try:
            # è·å–æ‰€æœ‰è¯­è¨€çš„ç½®ä¿¡åº¦
            lang_probs = detect_langs(text)

            if len(lang_probs) == 0:
                return "unknown", 0.0

            # æ£€æŸ¥æ˜¯å¦æœ‰å¤šç§è¯­è¨€
            if len(lang_probs) > 1:
                # æ£€æŸ¥ä¸­è‹±æ–‡æ··åˆ
                zh_prob = 0.0
                en_prob = 0.0

                for lang_prob in lang_probs:
                    if lang_prob.lang in ["zh", "zh-cn", "zh-tw", "zh-hk"]:
                        zh_prob += lang_prob.prob
                    elif lang_prob.lang == "en":
                        en_prob += lang_prob.prob

                # å¦‚æœä¸­è‹±æ–‡éƒ½æœ‰ä¸€å®šæ¯”ä¾‹ï¼Œè®¤ä¸ºæ˜¯æ··åˆè¯­è¨€
                if zh_prob > 0.2 and en_prob > 0.2:
                    return "mixed", max(zh_prob, en_prob)

            # å•ä¸€è¯­è¨€ï¼Œè¿”å›ä¸»è¦è¯­è¨€
            main_lang = lang_probs[0].lang
            confidence = lang_probs[0].prob

            lang_mapping = {
                "zh": "zh",
                "zh-cn": "zh",
                "zh-tw": "zh",
                "zh-hk": "zh",
                "en": "en",
                "ja": "ja",
                "ko": "ko",
            }

            detected_lang = lang_mapping.get(main_lang, main_lang)

            return detected_lang, confidence

        except LangDetectException:
            return "unknown", 0.0

    @staticmethod
    def get_optimal_ocr_configs(
        detected_language: str, confidence: float
    ) -> List[Dict[str, Any]]:
        """
        æ ¹æ®æ£€æµ‹åˆ°çš„è¯­è¨€ç±»å‹è·å–æœ€ä¼˜OCRé…ç½®

        Args:
            detected_language: æ£€æµ‹åˆ°çš„è¯­è¨€ç±»å‹
            confidence: è¯­è¨€æ£€æµ‹ç½®ä¿¡åº¦

        Returns:
            List[Dict[str, Any]]: æœ€ä¼˜OCRé…ç½®åˆ—è¡¨
        """
        # å¦‚æœç½®ä¿¡åº¦ä½äºé˜ˆå€¼ï¼Œä½¿ç”¨å›é€€é…ç½®
        if confidence < OCREngine.LANGUAGE_DETECTION_CONFIG["confidence_threshold"]:
            if OCREngine.LANGUAGE_DETECTION_CONFIG["fallback_to_mixed"]:
                return OCREngine.LANGUAGE_OCR_CONFIGS.get(
                    "mixed", OCREngine.SCAN_OCR_CONFIGS
                )
            else:
                return OCREngine.LANGUAGE_OCR_CONFIGS.get(
                    "fallback", OCREngine.SCAN_OCR_CONFIGS
                )

        # æ ¹æ®æ£€æµ‹åˆ°çš„è¯­è¨€é€‰æ‹©é…ç½®
        if detected_language == "zh":
            return OCREngine.LANGUAGE_OCR_CONFIGS.get("zh", OCREngine.SCAN_OCR_CONFIGS)
        elif detected_language == "en":
            return OCREngine.LANGUAGE_OCR_CONFIGS.get("en", OCREngine.SCAN_OCR_CONFIGS)
        elif detected_language == "mixed":
            return OCREngine.LANGUAGE_OCR_CONFIGS.get(
                "mixed", OCREngine.SCAN_OCR_CONFIGS
            )
        else:
            # æœªçŸ¥è¯­è¨€ï¼Œä½¿ç”¨å›é€€é…ç½®
            return OCREngine.LANGUAGE_OCR_CONFIGS.get(
                "fallback", OCREngine.SCAN_OCR_CONFIGS
            )

    @staticmethod
    def get_sample_text_for_detection(image) -> str:
        """
        ä»å›¾åƒä¸­æå–ç”¨äºè¯­è¨€æ£€æµ‹çš„æ ·æœ¬æ–‡æœ¬ï¼ˆå¢å¼ºç‰ˆï¼‰

        Args:
            image: PIL Imageå¯¹è±¡

        Returns:
            str: æ ·æœ¬æ–‡æœ¬
        """
        try:
            # ä½¿ç”¨å¿«é€ŸOCRé…ç½®è·å–æ ·æœ¬
            sample_config = OCREngine.LANGUAGE_DETECTION_CONFIG["sample_ocr_config"]
            custom_config = f'--psm {sample_config["psm"]} --dpi {sample_config["dpi"]}'

            # æ‰§è¡Œå¿«é€ŸOCR
            sample_text = pytesseract.image_to_string(
                image, lang=sample_config["lang"], config=custom_config
            )

            # æ¸…ç†å’Œæˆªå–æ ·æœ¬
            sample_text = sample_text.strip()

            # å¦‚æœæ–‡æœ¬å¤ªçŸ­ï¼Œå°è¯•ä½¿ç”¨æ›´é«˜DPIçš„é…ç½®
            if (
                len(sample_text)
                < OCREngine.LANGUAGE_DETECTION_CONFIG["min_text_length_for_detection"]
            ):
                # ä½¿ç”¨æ›´é«˜DPIçš„é…ç½®é‡æ–°æå–
                high_dpi_config = f'--psm {sample_config["psm"]} --dpi 200'
                high_dpi_text = pytesseract.image_to_string(
                    image, lang=sample_config["lang"], config=high_dpi_config
                )

                if len(high_dpi_text.strip()) > len(sample_text):
                    sample_text = high_dpi_text.strip()

            # å¦‚æœæ–‡æœ¬å¤ªé•¿ï¼Œåªå–å‰300ä¸ªå­—ç¬¦ï¼ˆå¢åŠ é•¿åº¦ï¼‰
            if len(sample_text) > 300:
                sample_text = sample_text[:300]

            # æ¸…ç†ç‰¹æ®Šå­—ç¬¦ï¼Œä¿ç•™ä¸­è‹±æ–‡å­—ç¬¦å’ŒåŸºæœ¬æ ‡ç‚¹
            sample_text = re.sub(r"[^\u4e00-\u9fff\w\s.,!?;:()ï¼ˆï¼‰]", "", sample_text)

            return sample_text

        except Exception as e:
            print(f"âš ï¸ æ ·æœ¬æ–‡æœ¬æå–å¤±è´¥: {e}")
            return ""

    @staticmethod
    def analyze_language_distribution(text: str) -> Dict[str, float]:
        """
        åˆ†ææ–‡æœ¬ä¸­çš„è¯­è¨€åˆ†å¸ƒ

        Args:
            text: å¾…åˆ†æçš„æ–‡æœ¬

        Returns:
            Dict[str, float]: è¯­è¨€åˆ†å¸ƒç»Ÿè®¡
        """
        if not text:
            return {}

        # ç»Ÿè®¡ä¸­æ–‡å­—ç¬¦
        chinese_chars = len(re.findall(r"[\u4e00-\u9fff]", text))

        # ç»Ÿè®¡è‹±æ–‡å­—ç¬¦
        english_chars = len(re.findall(r"[a-zA-Z]", text))

        # ç»Ÿè®¡æ•°å­—
        digits = len(re.findall(r"\d", text))

        # ç»Ÿè®¡å…¶ä»–å­—ç¬¦
        other_chars = len(text) - chinese_chars - english_chars - digits

        total_chars = len(text)

        if total_chars == 0:
            return {}

        return {
            "chinese": chinese_chars / total_chars,
            "english": english_chars / total_chars,
            "digits": digits / total_chars,
            "other": other_chars / total_chars,
            "total_chars": total_chars,
        }

    # ==================== æ–‡æ¡£åˆ†ææ–¹æ³• ====================

    @staticmethod
    def detect_document_type(image: Image.Image, sample_text: str) -> str:
        """
        æ ¹æ®æ–‡æ¡£ç‰¹å¾æ™ºèƒ½æ£€æµ‹æ–‡æ¡£ç±»å‹

        Args:
            image: PIL Imageå¯¹è±¡
            sample_text: æ ·æœ¬æ–‡æœ¬

        Returns:
            str: æ–‡æ¡£ç±»å‹ ("academic", "table", "chinese_only", "batch", "technical")
        """
        try:
            # 1. æ£€æµ‹è¡¨æ ¼ç»“æ„
            if OCREngine.has_table_structure(image):
                return "table"

            # 2. æ£€æµ‹å­¦æœ¯è®ºæ–‡ç‰¹å¾
            if OCREngine.has_academic_features(sample_text):
                return "academic"

            # 3. æ£€æµ‹çº¯ä¸­æ–‡æ–‡æ¡£
            if OCREngine.is_pure_chinese(sample_text):
                return "chinese_only"

            # 4. æ£€æµ‹æ‰¹é‡å¤„ç†éœ€æ±‚
            if OCREngine.is_batch_processing():
                return "batch"

            # 5. é»˜è®¤æŠ€æœ¯æ–‡æ¡£
            return "technical"

        except Exception as e:
            print(f"âš ï¸ æ–‡æ¡£ç±»å‹æ£€æµ‹å¤±è´¥: {e}")
            return "technical"  # é»˜è®¤å›é€€

    @staticmethod
    def has_table_structure(image: Image.Image) -> bool:
        """
        æ£€æµ‹å›¾åƒæ˜¯å¦åŒ…å«è¡¨æ ¼ç»“æ„

        Args:
            image: PIL Imageå¯¹è±¡

        Returns:
            bool: æ˜¯å¦åŒ…å«è¡¨æ ¼ç»“æ„
        """
        try:
            # è½¬æ¢ä¸ºOpenCVæ ¼å¼
            img_array = np.array(image)
            if len(img_array.shape) == 3:
                gray = cv2.cvtColor(img_array, cv2.COLOR_RGB2GRAY)
            else:
                gray = img_array

            # 1. è¾¹ç¼˜æ£€æµ‹
            edges = cv2.Canny(gray, 50, 150, apertureSize=3)

            # 2. ç›´çº¿æ£€æµ‹
            lines = cv2.HoughLinesP(
                edges,
                rho=1,
                theta=np.pi / 180,
                threshold=OCREngine.DOCUMENT_TYPE_DETECTION_CONFIG["table"][
                    "line_threshold"
                ],
                minLineLength=OCREngine.DOCUMENT_TYPE_DETECTION_CONFIG["table"][
                    "min_line_length"
                ],
                maxLineGap=OCREngine.DOCUMENT_TYPE_DETECTION_CONFIG["table"][
                    "max_line_gap"
                ],
            )

            if lines is None:
                return False

            # 3. åˆ†æç›´çº¿åˆ†å¸ƒ
            horizontal_lines = 0
            vertical_lines = 0

            for line in lines:
                x1, y1, x2, y2 = line[0]
                angle = abs(np.arctan2(y2 - y1, x2 - x1) * 180 / np.pi)

                if angle < 10 or angle > 170:  # æ°´å¹³çº¿
                    horizontal_lines += 1
                elif 80 < angle < 100:  # å‚ç›´çº¿
                    vertical_lines += 1

            # 4. åˆ¤æ–­æ˜¯å¦ä¸ºè¡¨æ ¼
            total_lines = len(lines)
            if total_lines == 0:
                return False

            horizontal_ratio = horizontal_lines / total_lines
            vertical_ratio = vertical_lines / total_lines

            # è¡¨æ ¼ç‰¹å¾ï¼šæ°´å¹³çº¿å’Œå‚ç›´çº¿éƒ½è¾ƒå¤š
            table_config = OCREngine.DOCUMENT_TYPE_DETECTION_CONFIG["table"]
            is_table = (
                horizontal_ratio > table_config["horizontal_ratio_threshold"]
                and vertical_ratio > table_config["vertical_ratio_threshold"]
                and total_lines >= table_config["min_lines"]
            )

            print(
                f"ğŸ“Š è¡¨æ ¼æ£€æµ‹: æ°´å¹³çº¿æ¯”ä¾‹={horizontal_ratio:.2f}, "
                f"å‚ç›´çº¿æ¯”ä¾‹={vertical_ratio:.2f}, æ€»çº¿æ¡æ•°={total_lines}"
            )
            return is_table

        except Exception as e:
            print(f"âš ï¸ è¡¨æ ¼ç»“æ„æ£€æµ‹å¤±è´¥: {e}")
            return False

    @staticmethod
    def has_academic_features(text: str) -> bool:
        """
        æ£€æµ‹æ–‡æœ¬æ˜¯å¦åŒ…å«å­¦æœ¯è®ºæ–‡ç‰¹å¾

        Args:
            text: å¾…æ£€æµ‹çš„æ–‡æœ¬

        Returns:
            bool: æ˜¯å¦åŒ…å«å­¦æœ¯è®ºæ–‡ç‰¹å¾
        """
        if not text:
            return False

        try:
            # 1. å­¦æœ¯è®ºæ–‡å…³é”®è¯æ£€æµ‹
            academic_keywords = OCREngine.DOCUMENT_TYPE_DETECTION_CONFIG["academic"][
                "keywords"
            ]
            keyword_matches = 0

            for keyword in academic_keywords:
                if keyword.lower() in text.lower():
                    keyword_matches += 1

            # 2. å¼•ç”¨æ ¼å¼æ£€æµ‹
            citation_patterns = OCREngine.DOCUMENT_TYPE_DETECTION_CONFIG["academic"][
                "citation_patterns"
            ]
            citation_matches = 0

            for pattern in citation_patterns:
                matches = re.findall(pattern, text)
                citation_matches += len(matches)

            # 3. ç« èŠ‚ç»“æ„æ£€æµ‹
            section_patterns = OCREngine.DOCUMENT_TYPE_DETECTION_CONFIG["academic"][
                "section_patterns"
            ]
            section_matches = 0

            for pattern in section_patterns:
                matches = re.findall(pattern, text)
                section_matches += len(matches)

            # 4. æ•°å­¦å…¬å¼æ£€æµ‹
            math_patterns = OCREngine.DOCUMENT_TYPE_DETECTION_CONFIG["academic"][
                "math_patterns"
            ]
            math_matches = 0

            for pattern in math_patterns:
                matches = re.findall(pattern, text)
                math_matches += len(matches)

            # 5. ç»¼åˆåˆ¤æ–­
            total_score = (
                keyword_matches
                * OCREngine.DOCUMENT_TYPE_DETECTION_CONFIG["academic"]["keyword_weight"]
                + citation_matches
                * OCREngine.DOCUMENT_TYPE_DETECTION_CONFIG["academic"][
                    "citation_weight"
                ]
                + section_matches
                * OCREngine.DOCUMENT_TYPE_DETECTION_CONFIG["academic"]["section_weight"]
                + math_matches
                * OCREngine.DOCUMENT_TYPE_DETECTION_CONFIG["academic"]["math_weight"]
            )

            is_academic = (
                total_score
                >= OCREngine.DOCUMENT_TYPE_DETECTION_CONFIG["academic"]["threshold"]
            )

            print(
                f"ğŸ“š å­¦æœ¯è®ºæ–‡æ£€æµ‹: å…³é”®è¯åŒ¹é…={keyword_matches}, å¼•ç”¨åŒ¹é…={citation_matches}, "
                f"ç« èŠ‚åŒ¹é…={section_matches}, å…¬å¼åŒ¹é…={math_matches}, æ€»åˆ†={total_score}"
            )
            return is_academic

        except Exception as e:
            print(f"âš ï¸ å­¦æœ¯è®ºæ–‡ç‰¹å¾æ£€æµ‹å¤±è´¥: {e}")
            return False

    @staticmethod
    def is_pure_chinese(text: str) -> bool:
        """
        æ£€æµ‹æ˜¯å¦ä¸ºçº¯ä¸­æ–‡æ–‡æ¡£

        Args:
            text: å¾…æ£€æµ‹çš„æ–‡æœ¬

        Returns:
            bool: æ˜¯å¦ä¸ºçº¯ä¸­æ–‡æ–‡æ¡£
        """
        if not text:
            return False

        try:
            # 1. å­—ç¬¦ç»Ÿè®¡
            chinese_chars = len(re.findall(r"[\u4e00-\u9fff]", text))
            english_chars = len(re.findall(r"[a-zA-Z]", text))
            total_chars = len(text.strip())

            if total_chars == 0:
                return False

            # 2. æ¯”ä¾‹è®¡ç®—
            chinese_ratio = chinese_chars / total_chars
            english_ratio = english_chars / total_chars

            # 3. åˆ¤æ–­æ¡ä»¶
            is_pure = (
                chinese_ratio
                >= OCREngine.DOCUMENT_TYPE_DETECTION_CONFIG["chinese_only"][
                    "chinese_ratio_threshold"
                ]
                and english_ratio
                <= OCREngine.DOCUMENT_TYPE_DETECTION_CONFIG["chinese_only"][
                    "english_ratio_threshold"
                ]
                and chinese_chars
                >= OCREngine.DOCUMENT_TYPE_DETECTION_CONFIG["chinese_only"][
                    "min_chinese_chars"
                ]
            )

            print(
                f"ğŸ‡¨ğŸ‡³ çº¯ä¸­æ–‡æ£€æµ‹: ä¸­æ–‡å­—ç¬¦æ¯”ä¾‹={chinese_ratio:.2f}, "
                f"è‹±æ–‡å­—ç¬¦æ¯”ä¾‹={english_ratio:.2f}, ä¸­æ–‡å­—ç¬¦æ•°={chinese_chars}"
            )
            return is_pure

        except Exception as e:
            print(f"âš ï¸ çº¯ä¸­æ–‡æ£€æµ‹å¤±è´¥: {e}")
            return False

    @staticmethod
    def is_batch_processing() -> bool:
        """
        æ£€æµ‹æ˜¯å¦ä¸ºæ‰¹é‡å¤„ç†æ¨¡å¼

        Returns:
            bool: æ˜¯å¦ä¸ºæ‰¹é‡å¤„ç†
        """
        # è¿™é‡Œå¯ä»¥æ ¹æ®å®é™…éœ€æ±‚å®ç°æ‰¹é‡å¤„ç†æ£€æµ‹
        # ä¾‹å¦‚ï¼šæ£€æŸ¥æ–‡ä»¶æ•°é‡ã€å¤„ç†æ¨¡å¼ç­‰
        # ç›®å‰è¿”å›Falseï¼Œåç»­å¯ä»¥æ ¹æ®éœ€è¦æ‰©å±•
        return False

    @staticmethod
    def analyze_document_features(
        image: Image.Image, sample_text: str
    ) -> Dict[str, Any]:
        """
        ç»¼åˆåˆ†ææ–‡æ¡£ç‰¹å¾

        Args:
            image: PIL Imageå¯¹è±¡
            sample_text: æ ·æœ¬æ–‡æœ¬

        Returns:
            Dict[str, Any]: æ–‡æ¡£ç‰¹å¾åˆ†æç»“æœ
        """
        try:
            features = {
                "has_table": OCREngine.has_table_structure(image),
                "has_academic": OCREngine.has_academic_features(sample_text),
                "is_pure_chinese": OCREngine.is_pure_chinese(sample_text),
                "is_batch": OCREngine.is_batch_processing(),
                "text_length": len(sample_text),
                "chinese_ratio": len(re.findall(r"[\u4e00-\u9fff]", sample_text))
                / max(len(sample_text), 1),
                "english_ratio": len(re.findall(r"[a-zA-Z]", sample_text))
                / max(len(sample_text), 1),
            }

            # ç¡®å®šæ–‡æ¡£ç±»å‹
            features["document_type"] = OCREngine.detect_document_type(
                image, sample_text
            )

            return features

        except Exception as e:
            print(f"âš ï¸ æ–‡æ¡£ç‰¹å¾åˆ†æå¤±è´¥: {e}")
            return {"document_type": "technical", "error": str(e)}

    # ==================== é…ç½®é€‰æ‹©æ–¹æ³• ====================

    @staticmethod
    def select_chinese_ocr_config(document_type: str) -> dict:
        """
        æ ¹æ®æ–‡æ¡£ç±»å‹æ™ºèƒ½é€‰æ‹©ä¸­æ–‡OCRé…ç½®

        Args:
            document_type: æ–‡æ¡£ç±»å‹

        Returns:
            dict: é€‰å®šçš„OCRé…ç½®
        """
        config_mapping = {
            "academic": OCREngine.CHINESE_OCR_BEST_PRACTICES["academic_paper"],
            "table": OCREngine.CHINESE_OCR_BEST_PRACTICES["table_document"],
            "chinese_only": OCREngine.CHINESE_OCR_BEST_PRACTICES["chinese_only"],
            "batch": OCREngine.CHINESE_OCR_BEST_PRACTICES["fast_processing"],
            "technical": OCREngine.CHINESE_OCR_BEST_PRACTICES["technical_doc"],
        }

        selected_config = config_mapping.get(
            document_type, OCREngine.DEFAULT_CHINESE_OCR_CONFIG
        )

        print(f"ğŸ¯ æ™ºèƒ½é…ç½®é€‰æ‹©: æ–‡æ¡£ç±»å‹={document_type}")
        print(f"ğŸ“‹ é€‰æ‹©é…ç½®: {selected_config['name']}")
        print(f"ğŸ“‹ é…ç½®æè¿°: {selected_config['description']}")
        print(f"ğŸ”§ PSMæ¨¡å¼: {selected_config['psm']}")
        print(f"ğŸ”§ DPIè®¾ç½®: {selected_config['dpi']}")
        print(f"ğŸ”§ è¯­è¨€æ¨¡å‹: {selected_config['lang']}")

        return selected_config

    # ==================== ä¾¿æ·è®¿é—®æ–¹æ³• ====================

    @staticmethod
    def get_scan_image_enhancement() -> Dict[str, Any]:
        """è·å–å›¾åƒå¢å¼ºé…ç½®"""
        return OCREngine.SCAN_IMAGE_ENHANCEMENT

    @staticmethod
    def get_scan_text_cleaning() -> Dict[str, Any]:
        """è·å–æ–‡æœ¬æ¸…ç†é…ç½®"""
        return OCREngine.SCAN_TEXT_CLEANING

    @staticmethod
    def get_scan_output_config() -> Dict[str, Any]:
        """è·å–è¾“å‡ºé…ç½®"""
        return OCREngine.SCAN_OUTPUT_CONFIG

    @staticmethod
    def get_output_format_config() -> Dict[str, Any]:
        """è·å–è¾“å‡ºæ ¼å¼é…ç½®"""
        return OCREngine.OUTPUT_FORMAT_CONFIG

    @staticmethod
    def get_default_chinese_ocr_config() -> Dict[str, Any]:
        """è·å–é»˜è®¤ä¸­æ–‡OCRé…ç½®"""
        return OCREngine.DEFAULT_CHINESE_OCR_CONFIG

    @staticmethod
    def get_default_english_ocr_config() -> Dict[str, Any]:
        """è·å–é»˜è®¤è‹±æ–‡OCRé…ç½®"""
        return OCREngine.DEFAULT_ENGLISH_OCR_CONFIG
